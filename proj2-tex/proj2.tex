\documentclass[twoside,11pt]{article}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

\usepackage{./jmlr2e}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}

% Definitions of handy macros can go here

\newcommand{\dataset}{{\cal D}}
\newcommand{\fracpartial}[2]{\frac{\partial #1}{\partial  #2}}

% Heading arguments are {volume}{year}{pages}{submitted}{published}{author-full-names}

\jmlrheading{1}{2020}{1-48}{4/00}{10/00}{Seth Bassetti, Ben Holmgren, and Wes Robbins}

% Short headings should be running head and authors last names

\ShortHeadings{Performance and Data Reduction For the K-Nearest-Neighbor Algorithm}{Bassetti, Holmgren, and Robbins}
\firstpageno{1}

\begin{document}

\title{Performance and Data Reduction For the K-Nearest-Neighbor Algorithm}

\author{\name Seth Bassetti \email seth.bassetti@student.montana.edu \\
       \addr School of Computing\\
       Montana State University\\
       Bozeman, MT, USA
	\AND
	\name Ben Holmgren \email benjamin.holmgren1@student.montana.edu \\
       \addr School of Computing\\
       Montana State University\\
       Bozeman, MT, USA
       \AND
       \name Wes Robbins \email wesley.robbins@student.montana.edu \\
       \addr School of Computing\\
       Montana State University\\
       Bozeman, MT, USA}
\editor{Seth Bassett, Ben Holmgren, and Wes Robbins}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
This paper describes our group's findings when utilizing various implementations
of the k-nearest-neighbor (KNN) algorithm for learning on various sets of data. 
 More specifically, we show how KNN performs on
 each set of data without any extra considerations, and after extra considerations
 following the edited nearest neighbor, condensed nearest neighbor, k-means, 
 and k-medoids. The performance of each of these respective models
 is evaluated in the context of both classification and regression. For datasets where
 classification was performed, the performance of each model was evaluated in terms of
 0/1 loss and logarithmic loss. For datasets where regression was performed,
 mean squared error and mean percent error were the chosen metrics to evaluate
 our implemented models. Prior to the project's completion, we hypothesized
that for every dataset, edited nearest neighbor, condensed nearest neighbor, and the two
clustering methods would cause at the very least a slight decrease in performance 
when compared with the performance of k-nearest-neighbor on only the underlying dataset.
 Namely, we expected to find
the largest runtime improvements on the largest datasets, and we also expected the
largest datasets to have the smallest decreases in performance as a result of 
utilizing these modifications to the data. Explicitly, we hypothesized that out of the data 
on which we performed classification, the house vote data (435 entries) would see the
largest runtime improvements with the lowest decrease in performance when
using the data modification methods mentioned above. Comparatively, the 
glass data and image segmentation data were
expected to perform without statistically significant differences when compared to
the house voting data. In terms of the data on which we performed regression, we
expected to find that the abalone data saw the greatest runtime
improvements from data modification and the lowest decrease in performance. 
We expected the forest fire data to observe statistically smaller runtime
improvements and statistically larger decreases in performance than the abalone data.
We expected the machine data to find runtime improvements which were
significantly smaller statistically than the other two regression datasets, with similarly
statistically significant reductions in the performance of the k-nearest-neighbor algorithm
after data reduction had occurred. In practice, we found a rather astonishing result that
data reduction at large didn't reduce the performance of any of the data sets with statistical
significance, and in general our hypothesis that larger datasets would perform better in the KNN
model was upheld.
\end{abstract}


\begin{keywords}
  K Nearest Neighbor, Edited Nearest Neighbor, Condensed Nearest Neighbor, K-Means, K-Medoids
\end{keywords}

\section{Problem Statement}
Utilizing six datasets- each from unique and differing settings, we implemented the k-nearest-neighbor
algorithm in our learning model as an attempt to provide insights into the correlation 
 of features within each set of data. More rigorously, we utilized k-nearest-neighbor to conduct supervised
 learning in two different forms for our data. In particular, three of the datasets are used to perform regression (Abalone, Computer
 Hardware, and Forest Fires),
  and three are used to perform classification (Glass, Vote, Image Segmentation). Each of the five datasets we use in the project have a variable 
  number of classes and either discrete or real valued attribute values. We hypothesized that in general, more data points would mean
  a better performance for our model, since more data could be interpreted as gaining greater specificity in spatially separating the classes, which
  is meaningful for the performance of k-nearest-neighbor. Further still, we hypothesized that the largest data sets in terms of the overall number of
  points would also find the best performance when it came to the performance of KNN following any kind of data reduction. Explicitly, this means that
  we would find for the classification data that the house vote data (with 435 entries) would perform the best on at least 90\% of runs, with and without
  data reduction. The glass and image segmentation data, with 214 and 215 entries respectively, we expected to find no statistically significant difference
  in performance. That is, neither data set would perform better than the other on at least 90\% of runs. Performance in this context was measurable by
  0/1 loss and logarithmic loss, with lower values in each corresponding to better performance of the model. For the regression data, we wanted to
  test the same hypothesis, which led us to believe that our best performing data would be the Abalone data set with 4,177 entries, and the Forest
  Fire data set with 518 points would perform worse than the Abalone data, but better than the Machine data with only 209 entries on 90\% of runs.
  Lastly, the Machine data would be the worst performer out of the regression data by our hypothesis, with and without reducing the size of the data.
  Performance in the context of regression was measurable by mean percent error and mean square error, both of which imply better performance with
  lower values.

\section{Methods}
In order to test our hypotheses, we ran our model using classification on the Glass, House Votes, and Image Segmentation data sets, and 
we ran our model using regression on the Abalone, Forest Fire, and Machine data. In order to successfully interpret the data, all feature values
which were not originally real valued were converted to real number values. The only data set with missing attribute values was the House Vote
data set, which included ``present" votes. We assigned all present votes equivalently to a ``no" vote, with the rationalization that a house member
would not abstain from voting for a motion if they were in favor of passing it. After all of the datasets had been preprocessed, we implemented the
KNN algorithm, which on a high level is as follows:
\begin{algorithm}
\begin{algorithmic}
\caption{\textsc{K-Nearest-Neighbor}}
\STATE \textbf{Input:} A set of vectors $X \subset \mathbb{R}^n$, each with a corresponding classification, an integer $k$, and a query point $\mathbb{X}_q$ whose class is unknown
\STATE \textbf{Output:} The estimated classification of $\mathbb{X}_q$, denoted $Class(\mathbb{X}_q)$
\STATE $\mathbb{K} \gets$ $k$ nearest neighbors to $\mathbb{X}_q$
\STATE $Class(\mathbb{X}_q) \gets \argmax{Classes(K)}$ 
\STATE \RETURN $Class(\mathbb{X}_q)$
\end{algorithmic}
\end{algorithm}

The implementation of \textsc{K-Nearest-Neighbor} then raises a number of questions in its implementation. Perhaps first and foremost, how is
distance computed? For our purposes, the Minkowski metric was utilized with a $p$ value set to $2$- i.e. the Euclidean distance was
what we chose for our KNN model. Letting $\mathbb{X}_i, \mathbb{X}_j$ denote two points and $d$ denote the dimension of the vector space,
the Minkoswki distance is computed with the following:
\begin{equation}
D(\mathbb{X}_i, \mathbb{X}_j) = (\sum_{l = 1}^{d} |\mathbb{X}_{il} - \mathbb{X}_{jl}|^p)^\frac{1}{p}
\end{equation}

Furthermore, in light of this distance metric, we also needed to construct a value difference metric \textit{(vdm)} to correctly process every column of
categorical data in order to actually use the Minkowski metric. To do this, for regression data, we computed the raw difference in frequencies
of each entry of categorical data, and used this as the numeric value to base distance from before inserting values into the vdm equation.
 For classification data, we simply insert values into the vdm equation. 
 TODO Ask sef what we're doin here.
 
 When computing distances between points, we also made use of a kernel function to smooth the densities of points and allow for weights to be
 considered in classification based on hw close other points are to a given query point. To do so, we computed a smooth kernel function using the following
 formula:
 \begin{equation}
 weight = (\frac{1}{\sqrt{2\pi}})^d * e(\frac{-1}{2} (\frac{dist}{band})^2)
 \end{equation}
 Here, we compute a continuous weight function to add a dimension to our data in order to smoothly incorporate density. In the equation we denote the dimension
 of an entry $d$, the distance between this query entry and its neighbor $dist$, and the tunable bandwidth parameter $band$. For our purposes, we chose to set
 $b=1$ after conducting some basic tests on our model, because it didn't seem to have any noticeable effect on our findings and meant that we would have one less
 variable to think about and justify throughout the rest of the project.
 
 Once a reliable distance metric was attained, we could compute $KNN$ for each of our 6 datasets. Lastly, there is the final remaining step of evaluating 
 the efficacy of KNN on each set of data. For classification data, we computed the logarithmic loss and 0/1 loss when our model attempted to guess
 classifications on the test set. 
These loss functions provide an effective method of determining model performance when certain features of the model are changed. Explicitly, letting $c_i$ be the level 
of certainty with which a guess was made in our model for the $i$th line of test data, logarithmic loss for each data point was computed as follows:
\[\begin{cases}
	ln(c_i) \text{ if correct}\\
	ln(1-c_i) \text{ if incorrect}\\
\end{cases}\]
Letting the value $y_i$ be the value produced by the above equation for the $i$th line of test data, the logarithmic loss  for the entire test set $T$ is computed
as:
\begin{equation}
\frac{-1}{|T|}\sum_{t \in T}^{ } y_i
\end{equation}

%1-correct/total
The second loss function we introduced was the 0/1 loss function, which is the ratio of incorrect guesses to total guesses made on the testing data.
Again denoting the test set $T$, and letting the number of correct guesses made on the test set by the model be denoted $g_c$, the 0/1 loss is computed with the simple ratio:
\begin{equation}
	1 - \frac{g_c}{|T|}
\end{equation}

We chose both of the above loss functions with the motivation that both are relatively simple to compute, as all of the terms are directly attainable from the output of our algorithm, and
because they are metrics with importantly opposed qualities in the evaluation of a simple KNN learning model. Perhaps the most famous and immediately logical evaluation of loss is 0/1 loss, 
which is a measurement of the ratio of incorrect guesses made in a test set. Importantly, this is indicative of the accuracy of our model, but is not particularly informative of overfitting.
Logarithmic loss however is helpful when evaluating the performance of a model while also keeping in mind the potential of overfitting. By incorporating punishments for having a high degree
of certainty in an incorrect decision, logarithmic loss is useful in capturing the performance of a model with greater depth than simply counting the number of correct solutions- 
which in turn provides a weariness for overfitting.
Both metrics are important to have an effective model, so both loss functions were chosen in the evaluation of our model on various data.

For regression data, we evaluated the performance of our model using mean percent error and mean squared error. Mean squared error is computed by
simply finding the mean of the squares of the errors. Mathematically, letting $Y$ be the vector of $n$ observed values and $\hat{Y}$ be the vector of $n$ predicted
values, mean squared error $MSE$ is computed as:
\begin{equation}
MSE = \frac{1}{n} \sum_{i=1}^{n} (Y_i - \hat{Y_i})^2
\end{equation}
Similarly, mean percent error $MPE$ is computed by finding the average percentage errors by which the forecasts of a model differ from the ground truth. Letting $a_t$ be the 
ground truth value of a prediction, $f_t$ be the actual value of the prediction, and $n$ being the number of different times a forecast is made, this is rigorously
computed as follows:
\begin{equation}
MPE = \frac{100}{n} \sum_{t=1}{n} \frac{a_t - f_t}{a_t}
\end{equation}

It is important to note that $KNN$ on a large dataset can be expensive in terms of runtime. To combat this, our group implemented a matrix to store all of the values of the distances
as defined above between each of the points in every dataset. In doing so, we were able to much more effectively test the success of our implementation of various algorithms, and to
gauge the overall performance of our model. While it is almost certain that faster implementations exist, we found that creating a matrix for each dataset to be the most effective
way to go forward throughout the project. The matrix autogenerates if it isn't already installed on a local machine, and generally takes around 20 minutes to a half an hour to install
for each dataset. It is also available on our github repository, and can be uploaded to a local machine by using \textit{git-lfs}. 

As a result, it can become important to reduce the size of data in some way in order to achieve k-nearest-neighbor with more reasonable time complexity. To reduce our data, we used
four different methods. The first two come down to choosing a direct subset smaller than the overall size of our dataset. These methods are the edited nearest neighbor and condensed
nearest neighbor algorithms. Intuitively, our implementation of the edited nearest neighbor algorithm comes in the form of simply deleting any entries which we guess incorrectly on the
first pass of $KNN$. As long as entries lead to incorrect guesses, we delete them, until the performance of the model stops improving or we stop making incorrect guesses.
In practice, we found that the vast majority of deletions (>95\%) in our model generally occurred in the first pass of the loop, and continually running the loop was found to be quite expensive in terms of runtime.
So for practical purposes, our experimentation on edited nearest neighbor was a result of just the first pass of the loop.
Algorithmically, edited nearest neighbor is as follows:
\begin{algorithm}
\begin{algorithmic}
\caption{\textsc{Edited Nearest Neighbor}}
\STATE \textbf{Input:} A set $\mathbb{X}$ of raw data
\STATE \textbf{Output:} A set $\mathbb{X}'$ of edited data
\STATE $\mathbb{X}^* \gets \emptyset$
\STATE $\mathbb{X}' \gets \emptyset$
\WHILE{$(|\mathbb{X}'|< |\mathbb{X}|)$ \textit{and} Performance does not degrade}
\FOR{$\mathbb{X}_i \in \mathbb{X}$}
\STATE $predicted \gets classify(\mathbb{X}_i)$
\IF{$predicted \not= \mathbb{X}_i.class$}
\STATE $\mathbb{X^*}_i \gets \mathbb{X}_i$
\ENDIF
\ENDFOR
\STATE $\mathbb{X'} \gets \mathbb{X}/\mathbb{X^*}$
\ENDWHILE
\end{algorithmic}
\end{algorithm}



\section{Results}
Todo

\section{Discussion}
Todo


\section{Summary}
Todo

\end{document}
